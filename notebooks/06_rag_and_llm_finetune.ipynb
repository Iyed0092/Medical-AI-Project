{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6715489c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Set paths\n",
    "PREPROCESSED_DIR = Path(\"./experiments/preprocessed\")\n",
    "MODEL_DIR = Path(\"./experiments/models/llm\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Synthetic medical documents\n",
    "documents = [\n",
    "    \"Patient has fever and cough, suggest chest X-ray\",\n",
    "    \"MRI shows lesion in frontal lobe\",\n",
    "    \"Blood pressure elevated, patient prescribed ACE inhibitor\",\n",
    "    \"Patient has shortness of breath and low oxygen saturation\",\n",
    "    \"CT scan shows pulmonary embolism\"\n",
    "]\n",
    "\n",
    "# Corresponding IDs\n",
    "doc_ids = np.arange(len(documents))\n",
    "\n",
    "# Synthetic queries\n",
    "queries = [\n",
    "    \"What does the MRI indicate?\",\n",
    "    \"Recommend treatment for high BP\",\n",
    "    \"Explain the X-ray findings\"\n",
    "]\n",
    "\n",
    "print(\"Synthetic medical documents and queries created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eaf01b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load sentence transformer for embeddings\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Compute embeddings for documents\n",
    "doc_embeddings = embed_model.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "# Build FAISS index\n",
    "embedding_dim = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197033d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_documents(query, top_k=2):\n",
    "    query_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    results = [documents[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "# Test retrieval\n",
    "for q in queries:\n",
    "    docs = retrieve_documents(q)\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    for i, d in enumerate(docs):\n",
    "        print(f\"Doc {i+1}: {d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51d390",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Simple generation example\n",
    "for q in queries:\n",
    "    retrieved_docs = retrieve_documents(q)\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    input_text = f\"Question: {q} Context: {context} Answer:\"\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    print(f\"Generated Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca0741",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\",\"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Wrap Flan-T5 model with LoRA\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.train()  # set to training mode\n",
    "\n",
    "# Dummy fine-tuning loop (for demonstration)\n",
    "optimizer = torch.optim.Adam(lora_model.parameters(), lr=1e-4)\n",
    "for epoch in range(1):\n",
    "    for q in queries:\n",
    "        retrieved_docs = retrieve_documents(q)\n",
    "        context = \" \".join(retrieved_docs)\n",
    "        input_text = f\"Question: {q} Context: {context} Answer:\"\n",
    "        target_text = \"Placeholder answer\"\n",
    "        \n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        labels = tokenizer(target_text, return_tensors=\"pt\").input_ids\n",
    "        outputs = lora_model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch}, Query: {q}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"LoRA fine-tuning demo completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
